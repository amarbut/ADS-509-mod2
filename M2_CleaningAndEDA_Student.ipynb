{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4855b28b",
   "metadata": {},
   "source": [
    "# ADS-509 Assignment 2.1\n",
    "## Text Cleaning and Exploration\n",
    "\n",
    "\n",
    "In this assignment, you will use the HackerNews dataset created in the Module 1 assignment to:\n",
    "- Clean, normalize, and tokenize text\n",
    "- Explore and analyze text\n",
    "- Vectorize text\n",
    "- Perform basic sentiment analysis\n",
    "  \n",
    "If you are not confident in the quality of your own dataset from Module 1, there is a clean dataset available for your use in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02944f0e",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
    "\n",
    "Work through this notebook as if it were a worksheet, completing the code sections marked with **TODO** in the cells provided. Similarly, written questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you to fill in with your answers. **Make sure to answer every question marked with a Q: for full credit**.\n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential import statements and make sure that all such statements are moved into the designated cell.\n",
    "\n",
    "A .pdf of this notebook, with your completed code and written answers, is what you should submit in Canvas for full credit. **DO NOT SUBMIT A NEW NOTEBOOK FILE OR A RAW .PY FILE**. Submitting in a different format makes it difficult to grade your work, and students who have done this in the past inevitably miss some of the required work or written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06415891-abbd-4ab4-ba59-decb25440a30",
   "metadata": {},
   "source": [
    "## Imports and Downloads\n",
    "\n",
    "We will be using some datasets from the NLTK library, so we need to make sure that these are downloaded correctly before trying to use them. Then we will import the rest of the libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "import nltk\n",
    "for res in ['punkt','punkt_tab','stopwords','vader_lexicon']:\n",
    "    nltk.download(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, string, random, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# set some parameters for our visualizations\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4288737",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Next we will load our dataset from Module 1 and double check that it is formatted correctly.\n",
    "\n",
    "If you are uncertain about your own dataset, or if you don't pass the check below, feel free to use the dataset provided on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fb986-0ef3-4acf-a0a6-b5a45fe73ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/module1/hn_comments_with_storymeta.csv'  # TODO: Update the file path as needed\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}. Update the path for your environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Rows:', len(df))\n",
    "expected_cols = {'comment_text','story_id','title','score','descendants','story_time'}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    print('Warning: missing expected columns:', missing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9f15",
   "metadata": {},
   "source": [
    "## Text Cleaning and Normalization\n",
    "\n",
    "Now we will clean up the text in the `comment_text` column of the dataset. There are many different cleaning steps that are common for text, depending on your data source and use case. For the purpose of this assignment, we will keep it fairly simple.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "Perform the following steps on the `comment_text` column:\n",
    "- Convert to lower case\n",
    "- Remove any URLs\n",
    "- Strip any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "def normalize_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    # TODO: convert text to lowercase, remove URLs from the text using the regex from above, and remove extra white space\n",
    "    ??\n",
    "    return s\n",
    "\n",
    "df['text_norm'] = df['comment_text'].apply(normalize_text)\n",
    "df[['comment_text','text_norm']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d8728",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "In natural language processing, tokenization is the process of splitting raw text into individual units for analysis. As you saw in this week's content, there are many different methods for tokenization, ranging in complexity. For this assignment, we will use the `nltk.word_tokenize` function as well as a manual regex-based function to tokenize our text into individual words.\n",
    "\n",
    "**TODO**:\n",
    "- Build a tokenizer use the `nltk.word_tokenize` function that returns a list of individual words.\n",
    "- Use the regex provided to build a tokenizer function that returns a list of individual words.\n",
    "\n",
    "**Q**: What are the default settings for the `nltk.word_tokenize` function, and do they make sense for this application?\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Q**: Do you see any differences between the two tokenization methods? What might be the cause of these differences?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c474f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tokenizer\n",
    "def tokenize_nltk(s):\n",
    "    # TODO: use the nltk.word_tokenize function to produce a list of tokens\n",
    "    return ??\n",
    "\n",
    "# Regex fallback (keeps alphanumerics and simple contractions)\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\")\n",
    "def tokenize_regex(s):\n",
    "    # TODO: use the regex provided to produce a list of tokens\n",
    "    return ??\n",
    "\n",
    "# Choose tokenizer (easy to switch for demos)\n",
    "df['nltk'] = df['text_norm'].apply(tokenize_nltk)\n",
    "df['regex'] = df['text_norm'].apply(tokenize_regex)\n",
    "df['nltk_n'] = df['nltk'].apply(len)\n",
    "df['regex_n'] = df['regex'].apply(len)\n",
    "df[['text_norm','nltk', 'nltk_n', 'regex', 'regex_n']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30aae0",
   "metadata": {},
   "source": [
    "## Stop Words and Punctuation Filtering\n",
    "\n",
    "Next we will remove stop words and punctuation from our `comment_text` column. We will use the nltk stopwords dataset, but feel free to add more words/tokens to the `CUSTOM_STOP` list below as you see fit.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "Build a function that will take our list of individual tokens as input to:\n",
    "- Remove all punctuation\n",
    "- Remove all stop words\n",
    "- Remove all numeric tokens (This does not mean removing all digits from the tokens, but removing tokens that are standalone numbers)\n",
    "\n",
    "**Q**: What are stop words and why is it useful to remove them? How would our analysis change if we did not remove stop words?\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Q**: What other cleaning steps or considerations might be a good idea in this or another dataset?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e06b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_STOP = set(stopwords.words('english'))\n",
    "CUSTOM_STOP = set(['nt', 'like', 'also', 'would', 'could', 'even', 'much']) # TODO: update this list as desired to get a more meaningful list of top words\n",
    "ALL_STOP = EN_STOP | CUSTOM_STOP\n",
    "\n",
    "def filter_tokens(tokens, drop_numbers=True):\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        # TODO: remove all punctuation, stop words, and numeric tokens from the list of tokens\n",
    "        tok = ??\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "df['tokens_clean'] = df['nltk'].apply(filter_tokens)\n",
    "df['n_tokens_clean'] = df['tokens_clean'].apply(len)\n",
    "df[['nltk','tokens_clean','nltk_n','n_tokens_clean']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e2d50",
   "metadata": {},
   "source": [
    "## N-grams and Visualizations\n",
    "\n",
    "In creating our list of individual words, we have created a dataset of *unigrams* or one-token units. It can be useful to look at larger units, such as *bi-grams* (two tokens) or *n-grams* (n tokens), for semantic analysis. Below, we will use unigram and bi-gram tokens to explore our dataset.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "In the cell provided, use the Pandas histogram function to produce a histogram for our `n_tokens_clean` column.\n",
    "\n",
    "**Q**: Compare the lists of unigrams and bigrams created below. Which would be more useful in describing the content of our dataset?\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Q**: In your opinion, is the wordcloud a useful visualization?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks to build a histogram of the n_tokens_clean column\n",
    "ax = ??\n",
    "ax.set_xlabel(??)\n",
    "ax.set_ylabel(??)\n",
    "ax.set_title(??)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5181844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common cleaned unigrams\n",
    "# optional TODO: update your CUSTOM_STOP list above if you see words on this list that you think should be removed with the stop words\n",
    "all_toks = [t for row in df['tokens_clean'] for t in row]\n",
    "cnt = Counter(all_toks)\n",
    "top_cnt = pd.DataFrame(cnt.most_common(30), columns=['token','count'])\n",
    "top_cnt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common bi-grams\n",
    "def bigrams(lst):\n",
    "    return list(zip(lst, lst[1:])) if len(lst) > 1 else []\n",
    "all_bi = []\n",
    "for row in df['tokens_clean']:\n",
    "    all_bi.extend(bigrams(row))\n",
    "bi_cnt = Counter(all_bi)\n",
    "top_bi = pd.DataFrame([(f\"{a} {b}\", c) for (a,b), c in bi_cnt.most_common(30)], columns=['bigram','count'])\n",
    "top_bi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a wordcloud\n",
    "wc = WordCloud(width=800, height=400, background_color='white')\n",
    "text_blob = ' '.join(all_toks[:200000])  # cap for speed\n",
    "img = wc.generate(text_blob).to_image()\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ad1ed",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "For many applications and analyses, we will need to represent our text data in a numerical fashion, similar to producing a one-hot encoding for a categorical variable. *Term frequency* (TF) and *term frequency-inverse document frequency* (TF-IDF) vectors are two of the more common methods for vectorizing text data. \n",
    "\n",
    "A TF vector represents a document (in our case a single comment) as one vector with each position representing a word/token in our dataset. The value of each position for each document is the number of times that word shows up in that document (term frequency).\n",
    "\n",
    "A TF-IDF vector is set up in the same way, but the value of each position is the number of times that word shows up in that document, divided by the number of documents that have the term.\n",
    "\n",
    "These vectors are often combined into a single matrix for analysis, called a document-term matrix (since one axis will have the individual documents, and one axis will have the individual words).\n",
    "\n",
    "**TODO**:\n",
    "- Use scikit‑learn’s `CountVectorizer` to create a TF document‑term matrix. This function has cleaning steps built into it, so we will apply the function to our `text-norm` column. Choose the appropriate settings to convert the text to lowercase, remove stopwords, ignore words that are in more than 95% of documents, and ignore words that are in fewer than 5 documents.\n",
    "- Use scikit‑learn’s `TfidfVectorizer` to create a TF-IDF document‑term matrix with the same cleaning settings as the TF matrix.\n",
    "\n",
    "**Q**: What benefit do we get from using TF-IDF instead of the raw TF matrix?\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Q**: What differences do you see between the TF and TF-IDF top terms shown below?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4628b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the CountVectorizer function to create a TF document-term matrix with the settings described above.\n",
    "cv = CountVectorizer(??)\n",
    "X_tf = cv.fit_transform(df['text_norm'].fillna(''))\n",
    "vocab = np.array(cv.get_feature_names_out())\n",
    "X_tf.shape, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the TfidfVectorizer function to create a TF-IDF document-term matrix with the settings described above.\n",
    "tfidf = TfidfVectorizer(??)\n",
    "X_tfidf = tfidf.fit_transform(df['text_norm'].fillna(''))\n",
    "tfidf_vocab = np.array(tfidf.get_feature_names_out())\n",
    "X_tfidf.shape, len(tfidf_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top tf anf tf-idf terms for a given doc\n",
    "def top_terms(row_vector, vocab, k=10):\n",
    "    row = row_vector.toarray().ravel()\n",
    "    idx = row.argsort()[::-1][:k]\n",
    "    return list(zip(vocab[idx], row[idx]))\n",
    "\n",
    "# Show top terms for 3 random comments\n",
    "for i in np.random.choice(X_tfidf.shape[0], size=3, replace=False):\n",
    "    print(f\"Doc {i} → top terms:\")\n",
    "    print(\"TF:\")\n",
    "    print(top_terms(X_tf[i], tfidf_vocab, k=10))\n",
    "    print(\"TF-IDF:\")\n",
    "    print(top_terms(X_tfidf[i], tfidf_vocab, k=10))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d3aef",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Sentiment analysis is used to produce a score for the \"sentiment\" of each document in your corpus. It is mosty frequently useful in applications in which you would like to understand the sentiment of a large corpus (e.g. are product reviews generally good or bad) or for segmenting a dataset for further analysis (e.g. within positive reviews, what topics are most common).\n",
    "\n",
    "Like tokenization and vectorization, sentiment analysis methods range greatly in their complexity. For this analysis we will be applying a static lexicon (VADER) to our text, which maps each word in the dataset to a sentiment score. These scores are then combined to produce a single score for each document--positive values indicate a positive sentiment, and negative values indicate a negative sentiment.\n",
    "\n",
    "**Q**: What do you notice about our distribution of sentiment scores? How would you expect this to change if we were looking at a dataset of Amazon product reviews?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the VADER sentiment lexicon to score our dataset\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "scores = df['text_norm'].fillna('').apply(sia.polarity_scores)\n",
    "df['sent_compound'] = scores.apply(lambda d: d['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7436cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "ax = df['sent_compound'].hist(bins=40)\n",
    "ax.set_xlabel('Compound sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of sentiment (VADER)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d508c",
   "metadata": {},
   "source": [
    "## Save Engineered Features\n",
    "We will save our modified dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['story_id','title','comment_id','user','text_norm','n_tokens_clean','sent_compound']].copy()\n",
    "OUT_DIR = 'data/module2' ## TODO: Update file path as needed\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, 'hn_comment_features.csv')\n",
    "features.to_csv(OUT_CSV, index=False)\n",
    "OUT_CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
